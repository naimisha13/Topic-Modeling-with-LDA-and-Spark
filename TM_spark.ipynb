{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fc4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "# for pre-processing\n",
    "from pyspark.sql.functions import regexp_replace, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#for time evaluation\n",
    "import time #changes unit - check why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5787a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running spark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963c52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for TF-IDF vectorization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a2b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 09:35:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#create spark session\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Topic Modeling\")\\\n",
    "            .config(\"spark.driver.extraClassPath\")\\\n",
    "            .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba56bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading the JSON file\n",
    "news_df = spark.read.json('News_Category_Dataset_v3.json') #reading json data\n",
    "\n",
    "# defining the categories that need to be preserved\n",
    "keep_categories = ['TECH', 'SPORTS', 'HEALTHY LIVING', 'STYLE', 'ENVIRONMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1de15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preserving the selected categories\n",
    "news_df = news_df.filter((news_df.category).isin(keep_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609d38a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "|             authors|   category|      date|            headline|                link|   short_description|row|\n",
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "|    Drew Costley, AP|ENVIRONMENT|2022-09-18|First Public Glob...|https://www.huffp...|On Monday, the wo...|  1|\n",
      "|BECKY BOHRER, MAR...|ENVIRONMENT|2022-09-17|Alaska Prepares F...|https://www.huffp...|“In 10 years, peo...|  2|\n",
      "|     DÁNICA COTO, AP|ENVIRONMENT|2022-09-17|Puerto Rico Brace...|https://www.huffp...|Puerto Rico was u...|  3|\n",
      "|   Nathalie Baptiste|ENVIRONMENT|2022-09-17|Privatization Isn...|https://www.huffp...|Studies have repe...|  4|\n",
      "|JULIE WATSON and ...|ENVIRONMENT|2022-09-10|Severe Winds Batt...|https://www.huffp...|After a 10-day he...|  5|\n",
      "|TERRY CHEA and OL...|ENVIRONMENT|2022-09-01|Toxic Algae Cause...|https://www.huffp...|The unprecedented...|  6|\n",
      "|       Hilary Hanson|ENVIRONMENT|2022-08-13|'Catastrophic Fai...|https://www.huffp...|University of Cal...|  7|\n",
      "|                    |ENVIRONMENT|2022-08-13|'Shocking' Fish D...|https://www.huffp...|Polish Prime Mini...|  8|\n",
      "|       Hilary Hanson|ENVIRONMENT|2022-08-06|New York Couple T...|https://www.huffp...|Zachary will be h...|  9|\n",
      "|Ken Ritter and Fe...|ENVIRONMENT|2022-07-30|Rain Cascades Fro...|https://www.huffp...|Parts of the parc...| 10|\n",
      "|   CURT ANDERSON, AP|ENVIRONMENT|2022-07-20|Chronic Starvatio...|https://www.huffp...|Last year, a reco...| 11|\n",
      "|Alexander C. Kaufman|ENVIRONMENT|2022-06-26|The Netherlands, ...|https://www.huffp...|This isn’t the co...| 12|\n",
      "|      Mary Papenfuss|ENVIRONMENT|2022-06-26|Man Killed By 11-...|https://www.huffp...|The giant reptile...| 13|\n",
      "|       Hilary Hanson|ENVIRONMENT|2022-06-25|Bear Looking For ...|https://www.huffp...|A black bear's de...| 14|\n",
      "| AMY BETH HANSON, AP|ENVIRONMENT|2022-06-14|Yellowstone Flood...|https://www.huffp...|Flooding has wipe...| 15|\n",
      "|      Mary Papenfuss|ENVIRONMENT|2022-06-03|Court Rules That ...|https://www.huffp...|“The court’s deci...| 16|\n",
      "|Alexander C. Kaufman|ENVIRONMENT|2022-05-22|The Lithium War N...|https://www.huffp...|A lithium mining ...| 17|\n",
      "|       Hilary Hanson|ENVIRONMENT|2022-05-21|Family Hears Nois...|https://www.huffp...|And that's why yo...| 18|\n",
      "|       Hilary Hanson|ENVIRONMENT|2022-05-07|Everybody Clap! E...|https://www.huffp...|Hawaiian monk sea...| 19|\n",
      "| Felicia Fonseca, AP|ENVIRONMENT|2022-05-01|Arizona Wildfire ...|https://www.huffp...|Ferocious winds t...| 20|\n",
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extracting 2000 headlines from each category \n",
    "windowDept = Window.partitionBy(\"category\").orderBy(col(\"date\").desc())\n",
    "df2=news_df.withColumn(\"row\",row_number().over(windowDept))\n",
    "news_df=df2.filter(col(\"row\") <= 2000)\n",
    "news_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d7339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "|             authors|   category|      date|            headline|                link|   short_description|row|\n",
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "|    Drew Costley, AP|ENVIRONMENT|2022-09-18|First Public Glob...|https://www.huffp...|On Monday, the wo...|  1|\n",
      "|BECKY BOHRER, MAR...|ENVIRONMENT|2022-09-17|Alaska Prepares F...|https://www.huffp...|“In 10 years, peo...|  2|\n",
      "|     DÁNICA COTO, AP|ENVIRONMENT|2022-09-17|Puerto Rico Brace...|https://www.huffp...|Puerto Rico was u...|  3|\n",
      "|   Nathalie Baptiste|ENVIRONMENT|2022-09-17|Privatization Isn...|https://www.huffp...|Studies have repe...|  4|\n",
      "|JULIE WATSON and ...|ENVIRONMENT|2022-09-10|Severe Winds Batt...|https://www.huffp...|After a 10-day he...|  5|\n",
      "+--------------------+-----------+----------+--------------------+--------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eedee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      category|count|\n",
      "+--------------+-----+\n",
      "|   ENVIRONMENT| 1444|\n",
      "|HEALTHY LIVING| 2000|\n",
      "|        SPORTS| 2000|\n",
      "|         STYLE| 2000|\n",
      "|          TECH| 2000|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the topic distribution\n",
    "news_df.groupBy(\"category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf85862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the headline column and creating documents dataframe\n",
    "col_name = 'headline'\n",
    "documents = news_df.select(col_name).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7faf39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9321\n"
     ]
    }
   ],
   "source": [
    "# checking the number of documents\n",
    "print(documents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9daf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the strings using regex\n",
    "#removing the special characters\n",
    "reg_exp1 = '[^A-Za-z0-9 ]'\n",
    "reg1 = regexp_replace(col(col_name), reg_exp1,\"\")\n",
    "\n",
    "documents1 = documents.select(col_name, \\\n",
    "    reg1.alias('clean_text'), \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09d83ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the words that are at the end and begining of some articles and don't contribute to the content\n",
    "reg_exp2 = ' PHOTOS?| VIDEOS?| WATCH| new| New| NEW'\n",
    "reg2 = regexp_replace(col(\"clean_text\"), reg_exp2,\"\")\n",
    "\n",
    "documents1 = documents1.select(col_name, \\\n",
    "    reg2.alias(\"clean_text1\"), \\\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11953d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for pre-processing the data: 7.737543106079102ms\n"
     ]
    }
   ],
   "source": [
    "#time for pre-processing the data\n",
    "elapsed_time1 = (time.time() - start)\n",
    "print('time for pre-processing the data: {}ms'.format(elapsed_time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13045358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data to pandas to perform tf-idf\n",
    "headlines = documents1.select('clean_text1').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f4ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since using tfidf, there isnt much difference in the results if stopwords are removed\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(headlines['clean_text1'])\n",
    "\n",
    "vocab = tf_vectorizer.vocabulary_ #vocabulary generated by CountVectorizer\n",
    "#print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c037d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain document representation form the count vectoriser sparse matrix\n",
    "docs = []\n",
    "for row in tf.toarray():\n",
    "    words = np.where(row != 0)[0].tolist()\n",
    "    words_count = []\n",
    "    for idx in words:\n",
    "        for count in range(row[idx]):\n",
    "            words_count.append(idx)\n",
    "    docs.append(words_count) # generating \n",
    "    \n",
    "# display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74713c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for pre-processing the data - TF-IDF: 1670596517.072829ms\n"
     ]
    }
   ],
   "source": [
    "#time for pre-processing the data - TF-IDF\n",
    "elapsed_time2 = (time.time() - elapsed_time1)\n",
    "print('time for pre-processing the data - TF-IDF: {}ms'.format(elapsed_time2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aba7e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Latent Directlet Allocation function\n",
    "\n",
    "# parameters - \n",
    "# docs = list of documents\n",
    "# vocab = the generated vocabulary\n",
    "# T = number of Topics\n",
    "# m = number of Topic words\n",
    "# itr_num = number of times the algorithm runs\n",
    "\n",
    "def LDA(docs, vocab, T, m, itr_num):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    N = len(docs)        # number of documents\n",
    "    V = len(vocab)  # size of the vocabulary \n",
    "    \n",
    "    \n",
    "    alpha = 1 / T         # Dirichlet prior for per-document topic distributions\n",
    "    beta = 1 / T        #Dirichlet prior for per-topic word distribution\n",
    "    \n",
    "    #innitializing the matrices\n",
    "    z_dn = [[0 for _ in range(len(d))] for d in docs]\n",
    "    t_dz = np.zeros((N, T))        # the theta matrix\n",
    "    p_zw = np.zeros((T, V))      # the phi matrix\n",
    "    nd = np.zeros((N))            # document array\n",
    "    nz = np.zeros((T))            # topics array\n",
    "\n",
    "\n",
    "    ## Initializing the parameters\n",
    "    for d, doc in enumerate(docs): #first pass over the corpus\n",
    "        # n = word id for the particular document\n",
    "        # w = global word id\n",
    "\n",
    "        for n, w in enumerate(doc):\n",
    "#             print(d,'----',doc)\n",
    "            # assigning a topic randomly to words\n",
    "            z_dn[d][n] = n % T \n",
    "            # getting the topic for word n in document d\n",
    "            z = z_dn[d][n]\n",
    "\n",
    "            # incermenting counts\n",
    "            nz[z] += 1\n",
    "            nd[d] += 1\n",
    "            t_dz[d][z] += 1\n",
    "        \n",
    "            p_zw[z, w] += 1\n",
    "            \n",
    "    for iteration in range(itr_num):\n",
    "\n",
    "        for d, doc in enumerate(docs):\n",
    "\n",
    "            for n, w in enumerate(doc):\n",
    "                # topic for word n in document d\n",
    "                z = z_dn[d][n]\n",
    "\n",
    "                # decrementing counts for word w with associated topic z\n",
    "                p_zw[z, w] -= 1\n",
    "                t_dz[d][z] -= 1\n",
    "                nz[z] -= 1\n",
    "\n",
    "                # sample new topic from according to the formula\n",
    "                p_d_t = (t_dz[d] + alpha) / (nd[d] - 1 + T * alpha)\n",
    "                p_t_w = (p_zw[:, w] + beta) / (nz + V * beta)\n",
    "                p_z = p_d_t * p_t_w\n",
    "                p_z /= np.sum(p_z)\n",
    "                new_z = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "                # setting z as the new topic and increment counts\n",
    "                z_dn[d][n] = new_z\n",
    "                t_dz[d][new_z] += 1\n",
    "                p_zw[new_z, w] += 1\n",
    "                nz[new_z] += 1\n",
    "\n",
    "                \n",
    "    #genarating and print the topic words\n",
    "    vocab_words = {value: key for key, value in vocab.items()} #vocab generated by countVectorizer is a dictionary\n",
    "    \n",
    "    for idx, topic in enumerate(p_zw):\n",
    "        topics = \"Topic #\"+ str(idx) +\": \"\n",
    "        topics += \" \".join([vocab_words[i] for i in topic.argsort()[:-m - 1:-1]])\n",
    "        print(topics)\n",
    "        \n",
    "    #time for pre-processing the data - TF-IDF\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    print('')\n",
    "    print('Time taken to run LDA for {} topics and {} topic words for {} iterations: {}s'\\\n",
    "          .format(T, m, itr_num, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d142a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: week apple photos iphone best fashion people 10 things day\n",
      "Topic #1: health facebook climate change just list cancer says red finds\n",
      "Topic #2: olympic olympics team winter gold make game rio time win\n",
      "\n",
      "Time taken to run LDA for 3 topics and 10 topic words for 10 iterations: 13.03121304512024s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 3, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f4c5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: week best photos animal day fashion things world 10 black\n",
      "Topic #1: climate change just health list gold olympic wins best beauty\n",
      "Topic #2: facebook trump climate like health world james change just fight\n",
      "Topic #3: nfl know heres help player red olympic star want style\n",
      "Topic #4: apple iphone people just olympics instagram 10 says week twitter\n",
      "\n",
      "Time taken to run LDA for 5 topics and 10 topic words for 10 iterations: 12.929238080978394s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 5, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c3b76f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: apple week photos million says\n",
      "Topic #1: climate change week apple iphone\n",
      "Topic #2: just best beauty fashion list\n",
      "Topic #3: health people facebook make women\n",
      "Topic #4: olympics olympic team rio gold\n",
      "\n",
      "Time taken to run LDA for 5 topics and 5 topic words for 10 iterations: 12.982478141784668s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 5, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c449b08b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: week red carpet 11 fashion\n",
      "Topic #1: change climate day best heres\n",
      "Topic #2: climate make beauty change big\n",
      "Topic #3: hair dont health people cancer\n",
      "Topic #4: just health people facebook mental\n",
      "Topic #5: apple iphone olympics facebook week\n",
      "Topic #6: photos olympic week animal nfl\n",
      "Topic #7: olympic team gold world zika\n",
      "Topic #8: game world says nba water\n",
      "Topic #9: world video twitter dead trumps\n",
      "\n",
      "Time taken to run LDA for 10 topics and 5 topic words for 10 iterations: 12.936904191970825s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 10, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28d11184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: week best fashion like photos day make look beauty things\n",
      "Topic #1: health olympic climate olympics change team gold says cancer rio\n",
      "Topic #2: apple facebook google iphone game twitter nfl just trump nba\n",
      "\n",
      "Time taken to run LDA for 3 topics and 10 topic words for 50 iterations: 64.3887619972229s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 3, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00caaf49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: best week fashion photos beauty style looks dress like red\n",
      "Topic #1: apple climate iphone change facebook says google super week nfl\n",
      "Topic #2: olympic olympics team gold rio win winter wins just nba\n",
      "Topic #3: health people dont trump care know need things life cancer\n",
      "Topic #4: world day video game google national future nfl major hurricane\n",
      "\n",
      "Time taken to run LDA for 5 topics and 10 topic words for 50 iterations: 65.23653292655945s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 5, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa44dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: week photos 10 best like beauty just things animal looks\n",
      "Topic #1: apple facebook climate iphone just change olympic heres says team\n",
      "Topic #2: fashion google olympics olympic just people time health world best\n",
      "\n",
      "Time taken to run LDA for 3 topics and 10 topic words for 5 iterations: 6.5506370067596436s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 3, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "884e692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 10 best apple beauty facebook week care health just day\n",
      "Topic #1: apple week iphone climate just change cancer best photos olympic\n",
      "Topic #2: world team fashion olympic week trump says people like google\n",
      "Topic #3: facebook says just like make look time twitter trump hair\n",
      "Topic #4: olympics people watch winter player health tech williams nfl online\n",
      "\n",
      "Time taken to run LDA for 5 topics and 10 topic words for 5 iterations: 6.560553073883057s\n"
     ]
    }
   ],
   "source": [
    "LDA(docs, vocab, 5, 10, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
