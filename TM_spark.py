
#importing the required libraries
# for pre-processing
from pyspark.sql.functions import regexp_replace, col, row_number
from pyspark.sql.window import Window


# In[2]:


# for running spark
import sys
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql import SQLContext


# In[3]:


#for TF-IDF vectorization
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer


# In[4]:


# #create spark session
# spark = SparkSession.builder             .appName("")            .config("spark.driver.extraClassPath")            .getOrCreate()
# sc = spark.sparkContext
# sc.setLogLevel('WARN')




if __name__ == "__main__":
  if len(sys.argv) !=2:
    print("Usage: spark-submit TM_spark.py <datafile>", file=sys.stderr) 
    sys.exit(-1)

spark = (SparkSession
           .builder
           .appName("Topic Modeling")
           .config("spark.driver.extraClassPath")
           .getOrCreate())

# print('lalalalala 1')
sc = spark.sparkContext
sc.setLogLevel('OFF')

# In[5]:

# print('lalalalala 2')

# reading the JSON file
news_df = spark.read.json(sys.argv[1]) #reading json data

# defining the categories that need to be preserved
keep_categories = ['TECH', 'SPORTS', 'HEALTHY LIVING', 'STYLE', 'ENVIRONMENT']


# print('lalalalala 3')
# In[6]:


#preserving the selected categories
news_df = news_df.filter((news_df.category).isin(keep_categories))


# In[7]:


#extracting 2000 headlines from each category 
windowDept = Window.partitionBy("category").orderBy(col("date").desc())
df2=news_df.withColumn("row",row_number().over(windowDept))
news_df=df2.filter(col("row") <= 2000)
# news_df.show()


# In[8]:


# view the topic distribution
# news_df.groupBy("category").count().show()


# In[9]:


#selecting the headline column and creating documents dataframe
col_name = 'headline'
documents = news_df.select(col_name).distinct()


# In[10]:


# checking the number of documents
# print(documents.count())


# In[11]:


#cleaning the strings using regex
#removing the special characters
reg_exp1 = '[^A-Za-z0-9 ]'
reg1 = regexp_replace(col(col_name), reg_exp1,"")

documents1 = documents.select(col_name,     reg1.alias('clean_text'),     )


# In[12]:


#removing the words that are at the end and begining of some articles and don't contribute to the content
reg_exp2 = ' PHOTOS?| VIDEOS?| WATCH| new| New| NEW'
reg2 = regexp_replace(col("clean_text"), reg_exp2,"")

documents1 = documents1.select(col_name,     reg2.alias("clean_text1"),     )


# In[13]:


#converting data to pandas to perform tf-idf
headlines = documents1.select('clean_text1').toPandas()


# In[14]:


# since using tfidf, there isnt much difference in the results if stopwords are removed
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                                max_features=10000,
                                stop_words='english')
tf = tf_vectorizer.fit_transform(headlines['clean_text1'])

vocab = tf_vectorizer.vocabulary_ #vocabulary generated by CountVectorizer
#print(vocab)


# In[15]:


# obtain document representation form the count vectoriser sparse matrix
docs = []
for row in tf.toarray():
    words = np.where(row != 0)[0].tolist()
    words_count = []
    for idx in words:
        for count in range(row[idx]):
            words_count.append(idx)
    docs.append(words_count) # generating 
    
# display(docs)


# In[16]:


# the Latent Directlet Allocation function

# parameters - 
# docs = list of documents
# vocab = the generated vocabulary
# T = number of Topics
# m = number of Topic words
# itr_num = number of times the algorithm runs

def LDA(docs, vocab, T, m, itr_num):
    
    N = len(docs)        # number of documents
    V = len(vocab)  # size of the vocabulary 
    
    
    alpha = 1 / T         # Dirichlet prior for per-document topic distributions
    beta = 1 / T        #Dirichlet prior for per-topic word distribution
    
    #innitializing the matrices
    z_dn = [[0 for _ in range(len(d))] for d in docs]
    t_dz = np.zeros((N, T))        # the theta matrix
    p_zw = np.zeros((T, V))      # the phi matrix
    nd = np.zeros((N))            # document array
    nz = np.zeros((T))            # topics array


    ## Initializing the parameters
    for d, doc in enumerate(docs): #first pass over the corpus
        # n = word id for the particular document
        # w = global word id
#         print('lalalalala')
        for n, w in enumerate(doc):
            # assigning a topic randomly to words
            z_dn[d][n] = n % T 
            # getting the topic for word n in document d
            z = z_dn[d][n]
#             print('*************')
            # incermenting counts
            nz[z] += 1
            nd[d] += 1
            t_dz[d][z] += 1
        
            p_zw[z, w] += 1
            
            
    #
    for iteration in range(itr_num):
#          dance = 1
        for d, doc in enumerate(docs):
#              dance += 1
            
            for n, w in enumerate(doc):
                # topic for word n in document d
                z = z_dn[d][n]
                # print(dance)
                # decrementing counts for word w with associated topic z
                p_zw[z, w] -= 1
                t_dz[d][z] -= 1
                nz[z] -= 1

                # sample new topic according to the formula
                p_d_t = (t_dz[d] + alpha) / (nd[d] - 1 + T * alpha)
                p_t_w = (p_zw[:, w] + beta) / (nz + V * beta)
                p_z = p_d_t * p_t_w
                p_z /= np.sum(p_z)
                z_new = np.random.multinomial(1, p_z).argmax()

                # setting z as the new topic and incrementing counts
                z_dn[d][n] = z_new
                t_dz[d][z_new] += 1
                p_zw[z_new, w] += 1
                nz[z_new] += 1
#             print(p_t_w) #p_zw) #t_dz) 
                
    #genarating and print the topic words
    vocab_words = {value: key for key, value in vocab.items()} #vocab generated by countVectorizer is a dictionary
    
    for idx, topic in enumerate(p_zw):
        topics = "Topic #"+ str(idx) +": "
        topics += " ".join([vocab_words[i] for i in topic.argsort()[:-m - 1:-1]])
        print(topics)


# In[18]:


LDA(docs, vocab, 3, 10, 10)


# In[17]:


# LDA(docs, vocab, 5, 10, 10)


# In[18]:


# LDA(docs, vocab, 5, 5, 10)


# In[19]:


# LDA(docs, vocab, 10, 5, 10)


# In[20]:


# LDA(docs, vocab, 3, 10, 50)


# In[21]:


# LDA(docs, vocab, 5, 10, 50)


# In[22]:


# LDA(docs, vocab, 3, 10, 5)


# In[23]:


# LDA(docs, vocab, 5, 10, 5)

spark.stop()

